{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPpJc0Yf5T1G5116NaX092b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhawesh2608/Shubham-s-Projects/blob/main/NLP_labFile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentence Tokenization:**\n",
        "\n",
        "### üß© What Is Sentence Tokenization?\n",
        "\n",
        "**Sentence tokenization** is a process in **Natural Language Processing (NLP)** where a large chunk of text is split into individual **sentences**. It's one of the first steps in analyzing or processing text data.\n",
        "\n",
        "\n",
        "### üîç Why Is It Useful?\n",
        "\n",
        "- Helps machines understand text structure\n",
        "- Enables further analysis like sentiment detection, translation, or summarization\n"
      ],
      "metadata": {
        "id": "isdEs_OQPgm0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7r1T40cLfgX",
        "outputId": "acf337b0-36b6-4402-f19f-0b73ad7103ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import  sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentence(text):\n",
        "    sentence = sent_tokenize(text)\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "sJkvT9S-Lt_h"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"He shaved the peach to prove a point.\n",
        "Italy is my favorite country; in fact,\n",
        "I plan to spend two weeks there next year.\n",
        "It's not possible to convince a monkey to give you\n",
        "a banana by promising it infinite bananas when they die.\n",
        "I made myself a peanut butter sandwich as\n",
        "I didn't want to subsist on veggie crackers.\n",
        "If you don't like toenails, you probably shouldn't look at your feet.\"\"\"\n",
        "\n",
        "sentences = tokenize_sentence(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcWdPKtcMZ0h",
        "outputId": "9245ede2-4ec6-4782-944e-7b46385a32ce"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He shaved the peach to prove a point.', 'Italy is my favorite country; in fact,\\nI plan to spend two weeks there next year.', \"It's not possible to convince a monkey to give you\\na banana by promising it infinite bananas when they die.\", \"I made myself a peanut butter sandwich as\\nI didn't want to subsist on veggie crackers.\", \"If you don't like toenails, you probably shouldn't look at your feet.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i ,sentence in enumerate(sentences):\n",
        "  print(f\"Sentence {i+1}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "supYfqarM37g",
        "outputId": "a07545d7-da21-4a8a-da9d-22a552f370b6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: He shaved the peach to prove a point.\n",
            "Sentence 2: Italy is my favorite country; in fact,\n",
            "I plan to spend two weeks there next year.\n",
            "Sentence 3: It's not possible to convince a monkey to give you\n",
            "a banana by promising it infinite bananas when they die.\n",
            "Sentence 4: I made myself a peanut butter sandwich as\n",
            "I didn't want to subsist on veggie crackers.\n",
            "Sentence 5: If you don't like toenails, you probably shouldn't look at your feet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Tokenization**\n",
        "Word tokenization is like teaching a computer to read by breaking text into its most basic building blocks‚Äî**words** üß±.\n",
        "\n",
        "### üß† What Is Word Tokenization?\n",
        "\n",
        "In **Natural Language Processing (NLP)**, **word tokenization** is the process of splitting a sentence or paragraph into individual words, called **tokens**. These tokens are the units that algorithms use to understand and analyze language.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### üîç Why It Useful\n",
        "\n",
        "- **Enables Text Anlysis** a Breaks down text so algorithms can analyze word frequency, patterns, and meaning.\n",
        "- **Simplifies Preprocessing Prepares** raw text for further steps like stemming, lemmatization, and part-of-speech tagging"
      ],
      "metadata": {
        "id": "ZlM3X8eYPf3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import  word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkLVXglYQq8U",
        "outputId": "4c530583-88fd-41f2-f873-a9891312e7d6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  def tokenize_words(text):\n",
        "    words = word_tokenize(text)\n",
        "    return words"
      ],
      "metadata": {
        "id": "EzaRMpKiQ4ys"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text  = \"\"\"\"Trash covered the landscape like sprinkles do a birthday cake.\n",
        "I cheated while playing the darts tournament by using a longbow.\"\"\"\n",
        "\n",
        "words  = tokenize_words(text)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDxnkSNrRNCc",
        "outputId": "501d71c1-4d8a-4f2d-f7bb-e2c6412beff3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['``', 'Trash', 'covered', 'the', 'landscape', 'like', 'sprinkles', 'do', 'a', 'birthday', 'cake', '.', 'I', 'cheated', 'while', 'playing', 'the', 'darts', 'tournament', 'by', 'using', 'a', 'longbow', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parts of Speech Tagging**\n",
        "\n",
        "üß† What is POS Tagging?\n",
        "\n",
        "Parts of Speech tagging is the process of labeling each word in a sentence with its **grammatical role**. It helps machines understand sentence structure and meaning.\n",
        "\n",
        "üè∑Ô∏è Common POS Tags\n",
        "* NN: Noun, singular\n",
        "* VB: Verb, base form\n",
        "* JJ: Adjective\n",
        "* RB: Adverb\n",
        "* DT: Determiner\n",
        "* IN: Preposition\n",
        "* PRP: Pronoun\n",
        "* CC: Coordinating conjunction\n",
        "* VBG: Verb, gerund/present participle\n",
        "* NNS: Noun, plural\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h05lDjrN4BAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt') ,('puntk_tab'),('average_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "EIfHH4HXRYur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a54de3f6-a82f-4c00-bdac-6befbc60902c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tagging(text):\n",
        "  words = word_tokenize(text)\n",
        "  tagged_words = nltk.pos_tag(words)\n",
        "  return tagged_words"
      ],
      "metadata": {
        "id": "-pXW79prypPK"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"He stepped gingerly onto the bridge knowing that enchantment awaited on the other side.\n",
        "The shooter says goodbye to his love.\n",
        "The beauty of the African sunset disguised the danger lurking nearby.\"\"\"\n",
        "\n",
        "tagged_text = pos_tagging(text)\n",
        "\n",
        "print(tagged_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKgDvfzUzUhR",
        "outputId": "69b1dc22-0dd1-4592-bc41-114bf3afd5a8"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('He', 'PRP'), ('stepped', 'VBD'), ('gingerly', 'RB'), ('onto', 'IN'), ('the', 'DT'), ('bridge', 'NN'), ('knowing', 'NN'), ('that', 'WDT'), ('enchantment', 'NN'), ('awaited', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('other', 'JJ'), ('side', 'NN'), ('.', '.'), ('The', 'DT'), ('shooter', 'NN'), ('says', 'VBZ'), ('goodbye', 'NN'), ('to', 'TO'), ('his', 'PRP$'), ('love', 'NN'), ('.', '.'), ('The', 'DT'), ('beauty', 'NN'), ('of', 'IN'), ('the', 'DT'), ('African', 'NNP'), ('sunset', 'NN'), ('disguised', 'VBD'), ('the', 'DT'), ('danger', 'NN'), ('lurking', 'VBG'), ('nearby', 'RB'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**\n",
        "\n",
        "üß† What is lemmatization\n",
        "\n",
        "Lemmatization is a key NLP technique that reduces words **to their base** or dictionary form‚Äîcalled a **lemma**‚Äîwhile considering the word‚Äôs context and part of speech. Think of it as smart **word simplification**\n",
        "\n",
        "üìö Lemmatization vs Stemming\n",
        "\n",
        "| Feature | Lemmatization üß† | Stemming üî™ |\n",
        "|---|---|---|\n",
        "| Uses context | ‚úÖ Yes | ‚ùå No |\n",
        "| Produces real words | ‚úÖ Yes | ‚ùå Often not |\n",
        "| Example: ‚Äúrunning‚Äù | ‚Üí ‚Äúrun‚Äù | ‚Üí ‚Äúrun‚Äù or ‚Äúrunn‚Äù |"
      ],
      "metadata": {
        "id": "Sh0SfBzS5m1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzGsD9TjzvZt",
        "outputId": "13fa1295-c4b4-43c7-c170-0be93c8dfed0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = word_tokenize(text)\n",
        "  lemmatized_text = ' '.join([lemmatizer.lemmatize(word)for word in tokens])\n",
        "  return lemmatized_text"
      ],
      "metadata": {
        "id": "7aYLMzfX0wgx"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The cats are chasing mice and playing in the garden\"\n",
        "\n",
        "lemmatized_text = lemmatize_text(text)\n",
        "print(\"Original Text:\",text)\n",
        "print(\"Lemmatize Text:\",lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo3P0SKF1Zv1",
        "outputId": "3d54daca-e86b-43dc-99cb-c0f456189c09"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The cats are chasing mice and playing in the garden\n",
            "Lemmatize Text: The cat are chasing mouse and playing in the garden\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rghcxnp22S4A"
      },
      "execution_count": 64,
      "outputs": []
    }
  ]
}